{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "MIN_WEEK_DATA = 10\n",
    "countries = ['INDIA', 'RUSSIA', 'SOUTHAFRICA', 'UK', 'US', 'CHINA', 'BRAZIL']\n",
    "country_labels = {}\n",
    "for idx, country in enumerate(countries):\n",
    "    country_labels[country] = idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in all the CSV data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data = False\n",
    "\n",
    "if 'country_data' not in locals():\n",
    "    locals()['country_data'] = {}\n",
    "    \n",
    "if load_data:    \n",
    "    for country in countries:\n",
    "        # List the contents of each extracted ZIP file\n",
    "        # Expected to be one directory per user; directory name = userID\n",
    "        users = os.listdir('../data/%s' % country)\n",
    "        country_data[country] = {}\n",
    "        for user_id in users:\n",
    "            csv_path = '../data/%s/%s/QueryResults.csv' % (country, user_id)\n",
    "            if os.path.exists(csv_path):\n",
    "                country_data[country][user_id] = pd.read_csv(csv_path)\n",
    "            else:\n",
    "                print('does not exist ' + csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expand the CSV panda dataframes by turning `creationdate` into a pandas `Timestamp` and adding `minute_of_week` and `week_of_year`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for country in country_data:\n",
    "    user_dfs = country_data[country]\n",
    "    for user_id in user_dfs:\n",
    "        user_df = user_dfs[user_id]\n",
    "        user_df['parsed'] = pd.to_datetime(user_df['creationdate'], format='%Y-%m-%d %H:%M:%S')\n",
    "        user_df['minute_of_week'] = user_df['parsed'].apply(lambda row: (row.dayofweek * 24 * 60) + (row.hour * 60) + (row.minute))\n",
    "        user_df['week_of_year'] = user_df['parsed'].apply(lambda row: row.weekofyear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `country_weekly_data` which is a list for every country containing week-by-week timestamps. This is built by looping through every data frame for every user. When we detect a difference in `week_of_year`, we start a new week of timestamps. \n",
    "\n",
    "\n",
    "**WARNING** this cell is very slow. There are functions in a cell below to help save the output for faster loading if the notebook crashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing INDIA\n",
      "processing RUSSIA\n",
      "processing SOUTHAFRICA\n",
      "processing UK\n",
      "processing US\n",
      "processing CHINA\n",
      "processing BRAZIL\n"
     ]
    }
   ],
   "source": [
    "process_data = False\n",
    "\n",
    "if 'country_weekly_data' not in locals():\n",
    "    locals()['country_weekly_data'] = {}\n",
    "\n",
    "if process_data:\n",
    "    for country in country_data:\n",
    "        user_dfs = country_data[country]\n",
    "        country_weekly_data[country] = []\n",
    "        print('processing %s' % country)\n",
    "        for user_id in user_dfs:\n",
    "            user_df = user_dfs[user_id]\n",
    "            last_week_of_year = None\n",
    "            cur_week = None\n",
    "            for index, row in user_df.iterrows():\n",
    "                # This is the start of a new week\n",
    "                if row['week_of_year'] != last_week_of_year:\n",
    "                    # Only add weeks with at least MIN_WEEK_DATA timestamps\n",
    "                    if cur_week and len(cur_week) > MIN_WEEK_DATA:\n",
    "                        country_weekly_data[country].append(list(cur_week))\n",
    "                    last_week_of_year = row['week_of_year']\n",
    "                    # We use a set here to prevent duplicates\n",
    "                    cur_week = set()\n",
    "                cur_week.add(row['minute_of_week'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing lots of users with timestamps at exactly midnight and no seconds (YYYY-MM-DD 00:00:00), this is suspicious. \n",
    "\n",
    "**TODO** investiage this more later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine how many weeks we have for each country. \n",
    "\n",
    "**CONCERN 1**: China, Russia and South Africa are low.\n",
    "\n",
    "**CONCERN 2**: The number of timestamps in each of these weeks probably varies a lot (US, UK, India probably have weeks with many timestamps; vice-versa for the lower countries)\n",
    "\n",
    "These concerns could lead to an overfit/poor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDIA 53228\n",
      "RUSSIA 18883\n",
      "SOUTHAFRICA 7485\n",
      "UK 86500\n",
      "US 94413\n",
      "CHINA 7246\n",
      "BRAZIL 12625\n"
     ]
    }
   ],
   "source": [
    "for country in country_weekly_data:\n",
    "    print('%s %d' % (country, len(country_weekly_data[country])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some functions to save or load the processed `country_weekly_data` to save time and memory for replaying the notebook. Uses a simple JSON format since the arrays are not massively huge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data = False\n",
    "load_data = True\n",
    "\n",
    "def save(arr, pth):\n",
    "    with open(pth, 'w') as fh:\n",
    "        fh.write(json.dumps(arr))\n",
    "        fh.flush()\n",
    "        os.fsync(fh.fileno())\n",
    "\n",
    "def load(pth):\n",
    "    with open(pth, 'r') as fh:\n",
    "        return json.loads(fh.read()):\n",
    "    \n",
    "if save_data:\n",
    "    for country in country_weekly_data:\n",
    "        save(country_weekly_data[country], '../data/processed/%s.json' % country)\n",
    "if load_data:\n",
    "    locals()['country_weekly_data'] = {}\n",
    "    for country in countries:\n",
    "        country_weekly_data[country] = load('../data/processed/%s.json' % country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we get our selection of what weeks we will train/test on. We naively grab the first 7500 of the larger datasets.\n",
    "\n",
    "**TODO**: check out the lengths of the weeks inside all the countries and determine which ones would be best to use to accomodate countries with thinner weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks_to_use = {\n",
    "    'CHINA': country_weekly_data['CHINA'],\n",
    "    'SOUTHAFRICA': country_weekly_data['SOUTHAFRICA'],\n",
    "    'INDIA': country_weekly_data['INDIA'][:7500],\n",
    "    'RUSSIA': country_weekly_data['RUSSIA'][:7500],\n",
    "    'US': country_weekly_data['US'][:7500],\n",
    "    'UK': country_weekly_data['UK'][:7500],\n",
    "    'BRAZIL': country_weekly_data['BRAZIL'][:7500],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we encode the weeks we are going to use into a format the neural networks will like. There are `10080` minutes in a week, so for each week we create an array of length `10080` filled with `0`s and set the indexes of the minutes a user is active to `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding CHINA\n",
      "Encoding SOUTHAFRICA\n",
      "Encoding INDIA\n",
      "Encoding RUSSIA\n",
      "Encoding US\n",
      "Encoding UK\n",
      "Encoding BRAZIL\n"
     ]
    }
   ],
   "source": [
    "encoded_weeks = {}\n",
    "for country in weeks_to_use:\n",
    "    week_data = weeks_to_use[country]\n",
    "    encoded_weeks[country] = []\n",
    "    print('Encoding ' + country)\n",
    "    for week in week_data:\n",
    "        encoded = np.zeros(10080, dtype=int)\n",
    "        for minute in week:\n",
    "            encoded[minute] = 1\n",
    "        encoded_weeks[country].append(encoded)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert encoded_weeks['US'][0][weeks_to_use['US'][0][0]] == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the labels for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for country in encoded_weeks:\n",
    "    weeks = encoded_weeks[country]\n",
    "    for week in weeks:\n",
    "        labels.append([country_labels[country]])\n",
    "one_hot_labels = keras.utils.to_categorical(labels, num_classes=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group all the data into one array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for country in encoded_weeks:\n",
    "    data += encoded_weeks[country]\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/test split the data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, one_hot_labels, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic single input classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "34994/34994 [==============================] - 6s 176us/step - loss: 1.6244 - acc: 0.3881\n",
      "Epoch 2/10\n",
      "34994/34994 [==============================] - 6s 166us/step - loss: 1.2992 - acc: 0.5405\n",
      "Epoch 3/10\n",
      "34994/34994 [==============================] - 5s 151us/step - loss: 1.1234 - acc: 0.6094\n",
      "Epoch 4/10\n",
      "34994/34994 [==============================] - 5s 146us/step - loss: 0.9951 - acc: 0.6596\n",
      "Epoch 5/10\n",
      "34994/34994 [==============================] - 6s 164us/step - loss: 0.8949 - acc: 0.6951\n",
      "Epoch 6/10\n",
      "34994/34994 [==============================] - 5s 157us/step - loss: 0.8143 - acc: 0.7270\n",
      "Epoch 7/10\n",
      "34994/34994 [==============================] - 5s 147us/step - loss: 0.7445 - acc: 0.7530\n",
      "Epoch 8/10\n",
      "34994/34994 [==============================] - 5s 143us/step - loss: 0.6846 - acc: 0.7752\n",
      "Epoch 9/10\n",
      "34994/34994 [==============================] - 5s 150us/step - loss: 0.6310 - acc: 0.7967\n",
      "Epoch 10/10\n",
      "34994/34994 [==============================] - 5s 146us/step - loss: 0.5842 - acc: 0.8139\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f52603ecba8>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_model = Sequential()\n",
    "basic_model.add(Dense(32, activation='relu', input_dim=10080))\n",
    "basic_model.add(Dense(7, activation='softmax'))\n",
    "basic_model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "basic_model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the basic model. It performs poorly with an accuracy of 36%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17237/17237 [==============================] - 1s 60us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.656821145766253, 0.3625920984016367]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_model.evaluate(X_test, y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more advanced model with multi-layer perceptrons and dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "34994/34994 [==============================] - 4s 126us/step - loss: 1.9350 - acc: 0.1710\n",
      "Epoch 2/20\n",
      "34994/34994 [==============================] - 4s 125us/step - loss: 1.8386 - acc: 0.2574\n",
      "Epoch 3/20\n",
      "34994/34994 [==============================] - 4s 110us/step - loss: 1.7056 - acc: 0.3166\n",
      "Epoch 4/20\n",
      "34994/34994 [==============================] - 4s 121us/step - loss: 1.6080 - acc: 0.3681\n",
      "Epoch 5/20\n",
      "34994/34994 [==============================] - 4s 111us/step - loss: 1.5240 - acc: 0.4034\n",
      "Epoch 6/20\n",
      "34994/34994 [==============================] - 4s 126us/step - loss: 1.4569 - acc: 0.4365\n",
      "Epoch 7/20\n",
      "34994/34994 [==============================] - 4s 117us/step - loss: 1.3848 - acc: 0.4655\n",
      "Epoch 8/20\n",
      "34994/34994 [==============================] - 5s 132us/step - loss: 1.3212 - acc: 0.4947\n",
      "Epoch 9/20\n",
      "34994/34994 [==============================] - 4s 117us/step - loss: 1.2581 - acc: 0.5198\n",
      "Epoch 10/20\n",
      "34994/34994 [==============================] - 4s 123us/step - loss: 1.1998 - acc: 0.5438\n",
      "Epoch 11/20\n",
      "34994/34994 [==============================] - 5s 129us/step - loss: 1.1373 - acc: 0.5730\n",
      "Epoch 12/20\n",
      "34994/34994 [==============================] - 4s 126us/step - loss: 1.0789 - acc: 0.5942\n",
      "Epoch 13/20\n",
      "34994/34994 [==============================] - 4s 114us/step - loss: 1.0268 - acc: 0.6134\n",
      "Epoch 14/20\n",
      "34994/34994 [==============================] - 4s 119us/step - loss: 0.9700 - acc: 0.6379\n",
      "Epoch 15/20\n",
      "34994/34994 [==============================] - 4s 124us/step - loss: 0.9193 - acc: 0.6575\n",
      "Epoch 16/20\n",
      "34994/34994 [==============================] - 5s 157us/step - loss: 0.8754 - acc: 0.6751\n",
      "Epoch 17/20\n",
      "34994/34994 [==============================] - 5s 133us/step - loss: 0.8307 - acc: 0.6954\n",
      "Epoch 18/20\n",
      "34994/34994 [==============================] - 5s 143us/step - loss: 0.7856 - acc: 0.7156\n",
      "Epoch 19/20\n",
      "34994/34994 [==============================] - 4s 125us/step - loss: 0.7490 - acc: 0.7325\n",
      "Epoch 20/20\n",
      "34994/34994 [==============================] - 5s 132us/step - loss: 0.7029 - acc: 0.7488\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f525850e080>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_model = Sequential()\n",
    "mlp_model.add(Dense(64, activation='relu', input_dim=10080))\n",
    "mlp_model.add(Dropout(0.5))\n",
    "mlp_model.add(Dense(64, activation='relu'))\n",
    "mlp_model.add(Dropout(0.5))\n",
    "mlp_model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "mlp_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "mlp_model.fit(X_train, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the MLP model. It performs slightly worse than the basic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17237/17237 [==============================] - 1s 64us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.1737916718961166, 0.35574635960472467]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_model.evaluate(X_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Analyze results, see where models are performing poorly.\n",
    "\n",
    "**TODO** More analysis of the week lengths for a given country, since none was done. Suspect that the countries with larger datasets have more timestamps in a week and the model is overfitting towards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stack_exchange",
   "language": "python",
   "name": "stack_exchange"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
